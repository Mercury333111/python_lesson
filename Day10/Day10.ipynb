{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d9d80-afe6-4790-af80-ddf0c71fad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Day 10 / 数据可视化进阶\n",
    "    词云图\n",
    "        文本处理\n",
    "            分词\n",
    "            删除停止词\n",
    "        词云生成\n",
    "    分词的应用：搜索引擎\n",
    "    情感极性分析\n",
    "    配色\n",
    "        色板网站\n",
    "        从图片生成色板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb6777b-2c0e-4793-8c0c-3d377103afd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (0.42.1)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.2.tar.gz (222 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m334.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from wordcloud) (1.24.2)\n",
      "Requirement already satisfied: pillow in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from wordcloud) (9.5.0)\n",
      "Requirement already satisfied: matplotlib in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.39.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/xiejiss/.pyenv/versions/3.10.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "\u001b[33m  DEPRECATION: wordcloud is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for wordcloud ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed wordcloud-1.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d138b7-8d7b-4a76-a9a1-5fb5897dbefb",
   "metadata": {},
   "source": [
    "### 词云\n",
    "\n",
    "词云（Word Cloud）是一种可视化方式，用于展示一段文本中频繁出现的词汇，以便于观察和分析。词云通过将文本中的词汇按照其出现频率或重要性进行可视化，将常见的词汇以较大的字体大小展示，而罕见的词汇则以较小的字体大小展示。这样，词云图形在一瞥之间能够传达文本的关键信息和主题。\n",
    "\n",
    "词云图通常由一个平铺的区域组成，其中包含了各种词汇。词汇的大小和颜色可以根据其在文本中的重要性或频率进行调整。通常情况下，出现频率较高的词汇会显示得更大、更醒目，而出现频率较低的词汇则显示得较小、较不显眼。\n",
    "\n",
    "词云图在文本分析、舆情监测、主题提取等领域广泛应用。它可以帮助人们快速了解文本的关键内容、主题倾向以及常见的关键词汇。通过可视化呈现词汇，词云图使得文本的分析和理解更加直观和易于理解。\n",
    "\n",
    "生成词云图的过程通常包括以下步骤：\n",
    "\n",
    "1. 文本预处理：对文本进行清洗、分词等预处理操作，去除无关词汇和噪声。\n",
    "2. 词频统计：统计每个词汇在文本中的出现频率。\n",
    "3. 词云生成：根据词频生成词云图，将常见词汇以较大字体展示，罕见词汇以较小字体展示。\n",
    "4. 可视化展示：将生成的词云图呈现给用户，以便观察和分析。\n",
    "\n",
    "#### 分词\n",
    "\n",
    "jieba库是一个流行的中文文本处理工具，用于中文分词。它是基于Python开发的，提供了高效、可靠的中文分词功能，并且易于使用。\n",
    "\n",
    "jieba库具有以下特点和功能：\n",
    "\n",
    "- 支持中文分词：jieba库的主要功能是将中文文本进行分词，将连续的中文字符序列切分为一个个有意义的词语。分词是中文自然语言处理的基础步骤，对于文本分析和处理非常重要。\n",
    "- 多种分词模式：jieba库支持多种分词模式，包括精确模式、全模式和搜索引擎模式。可以根据不同的需求选择合适的分词模式。\n",
    "- 支持自定义词典：jieba库允许用户加载自定义的词典，以增加分词准确性和适应特定领域的词汇。用户可以根据自己的需求添加专业术语、地名、人名等词汇，提高分词效果。\n",
    "- 并行分词：jieba库内部使用了多线程来加速分词处理，提高了分词的效率。\n",
    "- 支持繁体中文分词：jieba库可以处理繁体中文文本，并提供了针对繁体中文的分词功能。\n",
    "- 兼容性好：jieba库兼容Python 2.x和Python 3.x版本，可以在各种环境中使用。\n",
    "- 开源社区活跃：jieba库是开源的，代码托管在GitHub上。由于其易用性和高性能，它在中文文本处理领域得到了广泛的应用和支持，拥有活跃的开发者社区。\n",
    "\n",
    "总之，jieba库是一个强大且易用的中文分词工具，适用于各种中文文本处理任务，包括文本分析、机器学习、信息检索等。通过使用jieba库，可以方便地对中文文本进行分词处理，并进一步进行后续的文本分析和处理。\n",
    "\n",
    "---\n",
    "\n",
    "jieba库提供了两种主要的分词模式：精确模式（默认）和全模式。此外，还提供了一种搜索引擎模式，用于更精细的分词需求。\n",
    "\n",
    "- 精确模式（精确切分句子，适合文本分析）：该模式下，jieba会尽可能地将句子切分成精确的词语。它通过基于前缀词典实现最大匹配法来进行分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea1189fb-ce9e-4ade-a9b6-25873fb0fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 爱 自然语言 处理\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"我爱自然语言处理\"\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "seg_result = \" \".join(seg_list)\n",
    "print(seg_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855a0aa-3bc9-4439-98e7-1c8553fb92db",
   "metadata": {},
   "source": [
    "- 全模式（将句子中所有可能的词语都扫描出来，速度较快）：该模式下，jieba会将句子中所有可能的词语都进行切分，可能会产生冗余的词语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68714801-0c3f-4033-b0b2-1fa66de0a5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 爱 自然 自然语言 语言 处理\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"我爱自然语言处理\"\n",
    "seg_list = jieba.cut(text, cut_all=True)\n",
    "seg_result = \" \".join(seg_list)\n",
    "print(seg_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc06bfc-7b5e-4136-a0f9-d7b39a754831",
   "metadata": {},
   "source": [
    "- 搜索引擎模式（在精确模式的基础上，对长词再次切分）：该模式下，jieba会对较长的词语进行再次切分，提高召回率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf6bb3a8-0afb-4505-b552-a496df7ab71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 爱 自然 语言 自然语言 处理\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"我爱自然语言处理\"\n",
    "seg_list = jieba.cut_for_search(text)\n",
    "seg_result = \" \".join(seg_list)\n",
    "print(seg_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8198b03e-c4b0-463d-8778-4d96abfbb6e6",
   "metadata": {},
   "source": [
    "#### Recall 和 Accuracy\n",
    "\n",
    "##### Recall\n",
    "\n",
    "在信息检索领域，搜索引擎的召回率（Recall）是衡量搜索引擎检索结果覆盖率的指标。召回率表示在所有相关文档中，搜索引擎成功地检索到了多少个相关文档。\n",
    "\n",
    "具体而言，召回率是指检索到的相关文档数与所有相关文档数的比例。召回率的计算公式如下：\n",
    "\n",
    "Recall = 检索到的相关文档数 / 所有相关文档数\n",
    "\n",
    "召回率的取值范围是0到1之间，通常以百分比的形式表示。较高的召回率意味着搜索引擎能够更全面地找到相关文档，覆盖了更多的相关信息。\n",
    "\n",
    "##### Accuracy\n",
    "\n",
    "准确率（Accuracy）是指搜索引擎返回的相关文档中真正相关的文档数与返回的总文档数之间的比例。准确率衡量了搜索引擎返回的结果中有多少是正确的。\n",
    "\n",
    "具体而言，准确率的计算公式如下：\n",
    "\n",
    "Accuracy = 真正相关的文档数 / 返回的总文档数\n",
    "\n",
    "准确率的取值范围也是0到1之间，通常以百分比的形式表示。较高的准确率表示搜索引擎返回的结果中有更多是相关的文档，减少了误报的情况。\n",
    "\n",
    "准确率和召回率是相互关联的指标。提高准确率可能会导致召回率降低，因为为了减少误报，搜索引擎可能会过滤掉一些相关文档。提高召回率可能会导致准确率降低，因为搜索引擎可能会返回更多的非相关文档。\n",
    "\n",
    "在搜索引擎评估中，通常需要综合考虑准确率和召回率这两个指标。如果希望结果更准确，可以追求更高的准确率，但可能会降低召回率；如果希望结果更全面，可以追求更高的召回率，但可能会降低准确率。根据具体的应用场景和需求，可以根据准确率和召回率之间的平衡来选择最适合的搜索引擎设置和调优策略。\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "TF-IDF（Term Frequency-Inverse Document Frequency）是一种常用的文本特征表示方法，用于评估一个词语对于一个文档集合中的某个文档的重要程度或特征权重。\n",
    "\n",
    "TF（词频）指的是一个词语在**当前文档**中出现的频率。TF值越高，表示该词语在文档中出现得越频繁。\n",
    "\n",
    "IDF（逆文档频率）指的是一个词语在**整个文档集合**中的稀有程度。IDF值越高，表示该词语在整个文档集合中出现得越少，具有更高的独特性。\n",
    "\n",
    "TF-IDF的计算公式如下：\n",
    "\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "其中，TF-IDF值越高，表示该词语在**当前文档**中的重要性越高。这是因为，搜索引擎的核心目标是查找到相关的文档，而对区别文档最有意义的词语应该是那些在文档中出现频率高、而在整个文档集合的其他文档中出现频率少的词语。\n",
    "\n",
    "TF-IDF的作用是通过对词语的词频和文档频率进行加权，突出在特定文档中具有较高重要性的词语。常见的应用包括文本分类、信息检索、关键词提取等任务。在文本分类中，TF-IDF可以帮助识别出在某一类别中具有较高区分度的关键词；在信息检索中，TF-IDF可以用于计算查询词与文档的匹配程度，从而排序搜索结果。\n",
    "\n",
    "需要注意的是，TF-IDF方法并不考虑词语的语义信息，它只关注词频和文档频率。某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。\n",
    "\n",
    "### 删除停止词\n",
    "\n",
    "停止词（Stop words）是在文本处理和信息检索中指那些对文本含义没有重要贡献的常见词语，例如\"and\"、\"the\"、\"is\"等。这些词语在自然语言中经常出现，但通常不携带特定的语义信息或对文本分类、搜索等任务的结果产生显著影响。\n",
    "\n",
    "停止词通常是由语言的常用词汇组成的一个固定列表。具体的停止词列表可能因应用场景、语言和任务而异，因为某些词语在特定领域或任务中可能具有重要意义。\n",
    "\n",
    "在文本处理和信息检索任务中，删除停止词有以下几个主要原因：\n",
    "\n",
    "1. 减少数据噪音：停止词通常在文本中出现频率很高，但它们对于理解文本的内容和意义并没有太大帮助，只会增加数据的噪音。删除停止词可以减少冗余信息，使得后续文本处理和分析更加集中和有效。\n",
    "2. 减少存储空间和计算成本：停止词出现频率高，如果保留所有的停止词，会占用较大的存储空间和计算资源。通过删除停止词，可以减少数据的规模和复杂性，提高处理速度和效率。\n",
    "3. 提高关键信息的权重：在某些文本处理任务中，关键信息的权重对于结果的准确性和可靠性至关重要。删除停止词可以使得关键信息在文本表示中更加突出，提高重要信息的权重，有助于更好地捕捉文本的含义和特征。\n",
    "\n",
    "获取中文停止词列表一般可以通过以下两种途径：\n",
    "\n",
    "- 开源停止词库：有一些开源项目提供了中文停止词列表，可以直接下载和使用。其中比较常用的是哈工大停用词表、百度停用词表、中文停用词库等。你可以在这些项目的官方网站或代码库中找到相应的停止词列表文件。\n",
    "- 自然语言处理库：许多自然语言处理（NLP）库和工具包，如NLTK、spaCy、jieba等，提供了预定义的中文停止词列表。你可以使用这些库的内置功能来获取停止词列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc1a57-cce6-4f4d-b3d7-3aeb7c4c3c8c",
   "metadata": {},
   "source": [
    "#### 词云的实例\n",
    "\n",
    "> 词云（Word Cloud）是一种可视化方式，用于展示一段文本中频繁出现的词汇，以便于观察和分析。词云通过将文本中的词汇按照其出现频率或重要性进行可视化，将常见的词汇以较大的字体大小展示，而罕见的词汇则以较小的字体大小展示。这样，词云图形在一瞥之间能够传达文本的关键信息和主题。\n",
    "\n",
    "1. 获取文本\n",
    "2. 简单清洗\n",
    "3. 调用封装好的工具箱，生成词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac7eb8-a6a7-4a42-bdc5-e338def2ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from wordCloud import chnSegment\n",
    "from wordCloud import plotWordcloud\n",
    "\n",
    "script_folder = globals()['_dh'][0]\n",
    "print(script_folder)\n",
    "\n",
    "# 读取文件\n",
    "with open(path.join(script_folder, 'doc/example.txt')) as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 则先进行分词操作\n",
    "text = chnSegment.word_segment(text)  # 里面调用了 jieba\n",
    "    \n",
    "# 生成词云\n",
    "plotWordcloud.generate_wordcloud(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5351f762-ce0d-4184-9790-1c75cada59b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xiejiss/Code/python/python-preparatory-course/day10\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Notebook\n",
    "script_folder = globals()['_dh'][0]\n",
    "# Python Script\n",
    "# from os import path\n",
    "# script_folder = path.dirname(__file__)\n",
    "print(script_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0396aa0f-2cd2-40da-882c-1ce12a164a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xiejiss/Code/python/python-preparatory-course/day10/doc/example.txt\n"
     ]
    }
   ],
   "source": [
    "print(path.join(script_folder, 'doc/example.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b8d1a5-3b10-474c-be65-b462a8508fda",
   "metadata": {},
   "source": [
    "#### 文件所在目录和工作目录的区别\n",
    "\n",
    "文件所在目录（File Directory）是指文件在文件系统中的位置路径，表示文件在硬盘上的存储位置。它通常由文件名和文件路径组成，用于唯一标识文件在文件系统中的位置。\n",
    "\n",
    "工作目录（Working Directory），也称为当前目录（Current Directory），是指在操作系统中当前正在进行操作的进程所在的目录。它是操作系统提供的一个概念，表示进程在执行命令或操作时的参考目录。\n",
    "\n",
    "两者的区别在于：\n",
    "\n",
    "文件所在目录是文件在文件系统中的位置，是文件的存储位置信息。而工作目录是进程当前操作的参考目录，是操作系统用来解析相对路径的基准目录。\n",
    "\n",
    "文件所在目录是静态的，文件位置在创建时确定，并不会随着进程的改变而改变。而工作目录是动态的，随着进程的操作或切换而改变。\n",
    "\n",
    "文件所在目录是文件的属性，可以被其他进程或用户查询到。工作目录是进程的属性，对其他进程或用户不可见。\n",
    "\n",
    "在具体的操作中，文件所在目录和工作目录可以是相同的，也可以是不同的。当使用相对路径引用文件时，操作系统会根据当前的工作目录来解析路径，找到对应的文件。如果工作目录与文件所在目录不同，需要使用相对路径或绝对路径来准确定位文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "febfb824-e2c8-4ba5-8aea-cb540094706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录： /Users/xiejiss/Code/python/python-preparatory-course/day10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "working_directory = os.getcwd()\n",
    "\n",
    "print(\"当前工作目录：\", working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66578d71-7bc7-4b8d-b1fc-c905c62a5d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"script\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(\"./script/test.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "working_directory = os.getcwd()\n",
    "\n",
    "print(\"当前工作目录：\", working_directory)\n",
    "\n",
    "# Jupyter Notebook\n",
    "# script_folder = globals()['_dh'][0]\n",
    "# Python Script\n",
    "from os import path\n",
    "script_folder = path.dirname(__file__)\n",
    "print(\"文件所在目录：\", script_folder)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62bbc63f-39e7-4b08-bb54-138cb890d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录： /Users/xiejiss/Code/python/python-preparatory-course/day10\n",
      "文件所在目录： /Users/xiejiss/Code/python/python-preparatory-course/day10/./script\n"
     ]
    }
   ],
   "source": [
    "!python3 ./script/test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d017d-dc1c-4ca3-b2ff-906e9fec9d85",
   "metadata": {},
   "source": [
    "#### path.join\n",
    "\n",
    "path.join() 是 Python 中 os.path 模块中的一个函数，用于将多个路径片段拼接成一个完整的路径。\n",
    "\n",
    "path.join() 函数会根据操作系统的规则自动处理路径分隔符，确保生成的路径在不同操作系统上都是正确的。这使得代码在不同平台上的兼容性更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c440b1cf-ef68-46f4-b311-b41ba8bb1b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拼接后的完整路径： ./file.txt\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "\n",
    "path1 = \".\"\n",
    "path2 = \"file.txt\"\n",
    "\n",
    "full_path = path.join(path1, path2)\n",
    "\n",
    "print(\"拼接后的完整路径：\", full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3042d22-4e56-4639-80da-5159a6cf98ba",
   "metadata": {},
   "source": [
    "#### path.abspath\n",
    "\n",
    "path.abspath() 是 Python 中 os.path 模块中的一个函数，用于获取给定路径的绝对路径。\n",
    "\n",
    "path.abspath() 函数会将相对路径转换为绝对路径。如果给定的路径已经是绝对路径，则返回原始路径。\n",
    "\n",
    "需要注意的是，path.abspath() 函数只是返回给定路径的绝对路径字符串，并不会检查路径的存在性或有效性。它只是对路径字符串进行处理，确保返回的路径为绝对路径形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9feee72d-6f77-4e2b-9e39-a1edb6929c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拼接后的完整路径： /Users/xiejiss/Code/python/python-preparatory-course/day10/file.txt\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "\n",
    "path1 = path.abspath(\".\")\n",
    "path2 = \"file.txt\"\n",
    "\n",
    "full_path = path.join(path1, path2)\n",
    "\n",
    "print(\"拼接后的完整路径：\", full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307e3d2-cdfa-45c5-88fd-75d5a3ad6339",
   "metadata": {},
   "source": [
    "### 情感极性分析\n",
    "\n",
    "情感极性分析（Sentiment Polarity Analysis）是一种文本分析技术，用于确定给定文本表达的情感倾向或情感极性。它旨在自动判断文本的情感是正面、负面还是中性。\n",
    "\n",
    "情感极性分析的一般流程如下：\n",
    "\n",
    "1. 数据准备：收集或获取待分析的文本数据，可以是用户评论、社交媒体帖子、新闻文章等。\n",
    "2. 文本预处理：对文本数据进行预处理，包括分词、去除停止词、词干化或词形还原等步骤，以便更好地表示文本的含义和特征。\n",
    "3. 特征提取：从预处理后的文本中提取有意义的特征，常用的特征表示方法包括词袋模型（Bag of Words）、词频-逆文档频率（TF-IDF）等。\n",
    "4. 构建情感分类模型：使用标注好的情感数据集训练机器学习或深度学习模型，例如朴素贝叶斯、支持向量机、逻辑回归、循环神经网络（RNN）、卷积神经网络（CNN）等。\n",
    "5. 情感分类：将待分析的文本输入训练好的模型，进行情感分类预测。通常输出的情感极性标签为正面、负面或中性。\n",
    "6. 结果解释和评估：对分类结果进行解释和评估，可以使用评估指标如准确率、精确度、召回率、F1 值等来衡量模型的性能。\n",
    "\n",
    "但我们作为初学者，在这里可以直接调用已经编写好的工具箱，可以简单看看情感极性分析的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0b3f3f7-3645-4979-a1e7-59ee5267360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m187.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b29ca-af72-4479-a5d5-74a8f45e62d3",
   "metadata": {},
   "source": [
    "### 规则的解决思路\n",
    "\n",
    "1. 中文情感极性分析，文本切分为段落，再切词，通过情感词标识出各个词语的情感极性，包括积极、中立、消极。\n",
    "2. 结合句子结构（包括连词、否定词、副词、标点等）给各情感词语的情感极性赋予权重，然后加权求和得到文本的情感极性得分。\n",
    "3. 优点：泛化性好，规则可扩展性强，所有领域通用。\n",
    "4. 缺点：规则词典收集困难，专家系统的权重设定有局限，单一领域准确率相比模型方法低。\n",
    "\n",
    "### 模型的解决思路\n",
    "\n",
    "1. 常见的[NLP文本分类模型](https://github.com/shibing624/text-classifier)均可，包括经典文本分类模型（LR、SVM、Xgboost等）和深度文本分类模型（TextCNN、Bi-LSTM、BERT等）。\n",
    "2. 优点：单一领域准召率高。\n",
    "3. 缺点：不通用，有标注数据的样本收集困难，扩展性弱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c922f288-9a56-40c2-b427-ea3d400aa2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "苹果是一家伟大的公司 3.4346924811096997 {'score': 3.4346924811096997, 'sub_clause0': {'score': 3.4346924811096997, 'sentiment': [{'key': '苹果', 'adverb': [], 'denial': [], 'value': 1.37846341627, 'score': 1.37846341627}, {'key': '是', 'adverb': [], 'denial': [], 'value': -0.252600480826, 'score': -0.252600480826}, {'key': '一家', 'adverb': [], 'denial': [], 'value': 1.48470161748, 'score': 1.48470161748}, {'key': '伟大', 'adverb': [], 'denial': [], 'value': 1.14925252286, 'score': 1.14925252286}, {'key': '的', 'adverb': [], 'denial': [], 'value': 0.0353323193687, 'score': 0.0353323193687}, {'key': '公司', 'adverb': [], 'denial': [], 'value': -0.360456914043, 'score': -0.360456914043}], 'conjunction': []}}\n",
      "\n",
      "土豆丝很好吃 2.294311221077 {'score': 2.294311221077, 'sub_clause0': {'score': 2.294311221077, 'sentiment': [{'key': '土豆丝', 'adverb': [], 'denial': [], 'value': 0.294892711165, 'score': 0.294892711165}, {'key': '很', 'adverb': [], 'denial': [], 'value': 0.530242664632, 'score': 0.530242664632}, {'key': '好吃', 'adverb': [], 'denial': [], 'value': 1.46917584528, 'score': 1.46917584528}], 'conjunction': []}}\n",
      "\n",
      "土豆丝很难吃 -2.381874203563 {'score': -2.381874203563, 'sub_clause0': {'score': -2.381874203563, 'sentiment': [{'key': '土豆丝', 'adverb': [], 'denial': [], 'value': 0.294892711165, 'score': 0.294892711165}, {'key': '很', 'adverb': [], 'denial': [], 'value': 0.530242664632, 'score': 0.530242664632}, {'key': '难吃', 'adverb': [], 'denial': [], 'value': -3.20700957936, 'score': -3.20700957936}], 'conjunction': []}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pysenti\n",
    "\n",
    "texts = [\"苹果是一家伟大的公司\",\n",
    "         \"土豆丝很好吃\",\n",
    "         \"土豆丝很难吃\"]\n",
    "for i in texts:\n",
    "    r = pysenti.classify(i)\n",
    "    print(i, r['score'], r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7bcad50-53e1-4bf9-bcbf-49d896fe95d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "苹果是一家伟大的公司 {'positive_prob': 0.6819638428483796, 'negative_prob': 0.31803615715162037}\n",
      "\n",
      "土豆丝很好吃 {'positive_prob': 0.6008313281872775, 'negative_prob': 0.39916867181272253}\n",
      "\n",
      "垃圾，在酒店中应该是很差的！ {'positive_prob': 0.06056508633079882, 'negative_prob': 0.9394349136692012}\n",
      "\n",
      "我们刚走过一个烧烤店 {'positive_prob': 0.8248988123579972, 'negative_prob': 0.17510118764200278}\n",
      "\n",
      "土豆丝很难吃 {'positive_prob': 0.2831455391498756, 'negative_prob': 0.7168544608501244}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pysenti import ModelClassifier\n",
    "\n",
    "texts = [\"苹果是一家伟大的公司\",\n",
    "         \"土豆丝很好吃\",\n",
    "         \"垃圾，在酒店中应该是很差的！\",\n",
    "         \"我们刚走过一个烧烤店\",\n",
    "         \"土豆丝很难吃\"]\n",
    "\n",
    "m = ModelClassifier()\n",
    "for i in texts:\n",
    "    r = m.classify(i)\n",
    "    print(i, r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22541a-57fb-4dcb-bba5-9956756fc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用情感极性分析，分析电影的一句话评语\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea6a20-c5ab-4c01-96db-0d8454c4e919",
   "metadata": {},
   "source": [
    "### 配色"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af5d0fd-3e40-4c38-9bdf-aef72bfbf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b6d662-a7d2-48ef-802c-f30246e4d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<colorgram.py Color: Rgb(r=219, g=230, b=244), 50.51671056314275%>, <colorgram.py Color: Rgb(r=119, g=174, b=210), 24.063880735547055%>, <colorgram.py Color: Rgb(r=34, g=108, b=159), 7.934377758480871%>, <colorgram.py Color: Rgb(r=175, g=186, b=220), 5.96047164191958%>, <colorgram.py Color: Rgb(r=213, g=243, b=237), 5.880812096768406%>, <colorgram.py Color: Rgb(r=249, g=248, b=244), 5.64374720414134%>]\n",
      "Rgb(r=219, g=230, b=244) Hsl(h=151, s=135, l=231) 0.5051671056314275 219 230 244\n"
     ]
    }
   ],
   "source": [
    "colors = colorgram.extract('example.jpg', 6)\n",
    "print(colors)\n",
    "\n",
    "first_color = colors[0]\n",
    "rgb = first_color.rgb\n",
    "hsl = first_color.hsl\n",
    "proportion = first_color.proportion\n",
    "\n",
    "r = first_color.rgb.r\n",
    "g = first_color.rgb.g\n",
    "b = first_color.rgb.b\n",
    "print(rgb, hsl, proportion, r, g, b)\n",
    "\n",
    "# 生成 HTML 展示图片和所有颜色\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5291a14-a276-4db3-ad0d-cac6bf0fb92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
